{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the 歌词 from GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv \n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<H>在北京的心脏 高耸钟楼的钟声在回荡\n",
      "<H>这是飞翔的聚地 清华大学梦的光芒\n",
      "<H>承载知识与智慧 燃烧着文化的热烈火花\n",
      "<H>你的名字是繁荣的象征 清华大学砥砺前行的力量\n",
      "<H>历史的风雨中 你肩上的责任就是希望\n",
      "<H>清华大学你是明灯 照亮每个孤独的夜晚\n",
      "<H>在这片梦想的土地 冲破黑暗的束缚\n",
      "<H>清华大学你是儿女们 心中的楷模\n",
      "<H>向你致敬 清华大学 你是我们内心的宇宙\n",
      "<H>带着梦想向前行 清华大学，我们再不曾孤独。\n",
      "在北京的心脏\n",
      "高耸钟楼的钟声在回荡\n",
      "这是飞翔的聚地\n",
      "清华大学梦的光芒\n",
      "承载知识与智慧\n",
      "燃烧着文化的热烈火花\n",
      "你的名字是繁荣的象征\n",
      "清华大学砥砺前行的力量\n",
      "历史的风雨中\n",
      "你肩上的责任就是希望\n",
      "清华大学你是明灯\n",
      "照亮每个孤独的夜晚\n",
      "在这片梦想的土地\n",
      "冲破黑暗的束缚\n",
      "清华大学你是儿女们\n",
      "心中的楷模\n",
      "向你致敬\n",
      "清华大学\n",
      "你是我们内心的宇宙\n",
      "带着梦想向前行\n",
      "清华大学，我们再不曾孤独。\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_response(client, prompt, model=\"gpt-4\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# get_response(client, \"gpt-4\", \"Compose a song for Tsinghua University. Only show the lyrics. Use <H> to split lines.\")\n",
    "\n",
    "def withoutRoman(item):\n",
    "    return not bool(re.search(r'[A-Za-z0-9\\(\\)]', item))\n",
    "\n",
    "def get_lyric_list(result):\n",
    "    lyrics = [item.strip() for item in result.split(\"<H>\")]\n",
    "    new_lyrics = []\n",
    "    for item in lyrics:\n",
    "        new_lyrics.extend(item.split())\n",
    "    lyrics = [item for item in new_lyrics if len(item) > 0]\n",
    "    lyrics = list(filter(withoutRoman, lyrics))\n",
    "    return lyrics\n",
    "\n",
    "result = get_response(client, \"为清华大学创作一首歌曲。只显示歌词。只用逗号或句号断句。使用<H>分割行，不超过10句歌词。\", \"gpt-4\")\n",
    "print(result)\n",
    "lyrics = get_lyric_list(result)\n",
    "print(\"\\n\".join(lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the images according to the 歌词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_image_dalle(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1):\n",
    "  response = client.images.generate(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    size=size,\n",
    "    quality=quality,\n",
    "    n=n,\n",
    "  )\n",
    "\n",
    "  image_url = response.data[0].url\n",
    "  response = requests.get(image_url, timeout=5000)\n",
    "  img = Image.open(BytesIO(response.content))\n",
    "  img_np = np.array(img)\n",
    "  img = Image.fromarray(img_np)\n",
    "  \n",
    "  return img\n",
    "\n",
    "def get_image_from_lyrics(lyrics_list, sleep_time=1):\n",
    "  # sleep time for avoiding rate limit\n",
    "  img_list = []\n",
    "  for lyric in lyrics_list:\n",
    "    img = get_image_dalle(lyric)\n",
    "    img_list.append(img)\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "# save image:\n",
    "# img.save(image_path) with postfix: .png/.jpg\n",
    "# img, = get_image_dalle('清华大学学生的殿堂', model='dall-e-2', size='256x256', quality='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 加上字幕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write characters onto the image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "\n",
    "def get_average_brightness(img):\n",
    "    average_brightness = img.sum() / img.size / 255\n",
    "    return average_brightness\n",
    "\n",
    "# 智能判断应该用黑字幕或者白字幕\n",
    "def should_use_white_or_black_text(img):\n",
    "    average_brightness = get_average_brightness(img)\n",
    "    # print(average_brightness)\n",
    "    # 假设亮度大于0.5（即128）时背景较亮，使用黑色字幕\n",
    "    if average_brightness > 0.5:\n",
    "        return (0, 0, 0)\n",
    "    else:\n",
    "        return (255, 255, 255)\n",
    "    \n",
    "def write_text_to_image(img, text, color=(255, 255, 255), color_func=None, font_size=20, font_path=\"/root/STLITI.TTF\", clone=True):\n",
    "    # 以隶书为例，需要上传字体文件\n",
    "    # clone = False 则在img本身上绘制，否则会创建一个新的副本\n",
    "    # color_func: 判断字幕颜色的函数\n",
    "    if clone:\n",
    "        img = img.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    size_x, size_y = bbox[2], bbox[3]\n",
    "    # 默认下侧距离边缘1个字高的正中地方绘制歌词\n",
    "    img_x, img_y = img.size[0], img.size[1]\n",
    "    if color_func:\n",
    "        color = color_func(np.array(img)[img_y - font_size - size_y: img_y - font_size, (img_x - size_x)// 2:(img_x + size_x)// 2, :])\n",
    "    draw.text(((img_x - size_x)// 2, img_y - font_size - size_y), text, color, font=font)\n",
    "    return img\n",
    "\n",
    "# img_new = write_text_to_image(img, \"清华大学是学生的摇篮\", color_func=should_use_white_or_black_text)\n",
    "# img_new.save(\"/root/new.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imageio\n",
    "# !pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# 设置生成的视频文件名和路径\n",
    "\n",
    "def process_image(file_name, postfix=\".jpg\"):\n",
    "    if file_name.endswith(postfix):\n",
    "        image = Image.open(file_name)\n",
    "        frame = image.convert(\"RGB\")\n",
    "        frame = np.array(frame.getdata()).reshape(frame.size[0], frame.size[1], 3)\n",
    "    return frame\n",
    "def generate_video(file_path=\"output.mp4\", image_files_dir=\".\", postfix=\".jpg\", fps=2):\n",
    "    # fps: frame/second\n",
    "    # read images in a directory\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # 寻找所有 png 文件\n",
    "        image_files = [os.path.join(image_files_dir, file) for file in os.listdir(image_files_dir) if file.endswith(postfix)]\n",
    "        # 利用线程池并行处理图像\n",
    "        images = list(executor.map(process_image, image_files))\n",
    "    # 将图片转换为视频文件\n",
    "    with imageio.get_writer(file_path, fps=fps) as video:\n",
    "        for image in images:\n",
    "            video.append_data(image)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get the 歌曲 of each line of 歌词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set max length to 2048\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006167411804199219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e184cfbe146caabd6fdf0aa0a6986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "ckpt_path = \"Mar2Ding/songcomposer_sft\" # your path\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path, trust_remote_code=True)\n",
    "# 先half再cuda\n",
    "model = AutoModel.from_pretrained(ckpt_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "def inference(model, tokenizer, question, device=\"cuda\"):\n",
    "    print(question)\n",
    "    question = f'[UNUSED_TOKEN_146]user\\n{question}[UNUSED_TOKEN_145]\\n'\n",
    "    stop_words_ids = [ \n",
    "                    torch.tensor([2]).cuda(), #'</s>'\n",
    "                    torch.tensor([92542]).cuda(), #'[UNUSED_TOKEN_145]'\n",
    "                    ]\n",
    "    stopping_criteria = StoppingCriteriaList(\n",
    "            [StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "\n",
    "    d = f\"{question}\"\n",
    "    input_ids = tokenizer(d, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids([\"[UNUSED_TOKEN_145]\"])[0]]\n",
    "    with torch.no_grad():\n",
    "        generate = model.generate(input_ids.to(device), \n",
    "                                    do_sample=True,\n",
    "                                    temperature=1.0,\n",
    "                                    repetition_penalty=1.005, \n",
    "                                    max_new_tokens=1000, \n",
    "                                    top_p=0.8, \n",
    "                                    top_k=50, \n",
    "                                    eos_token_id=eos_token_id,\n",
    "                                    stopping_criteria=stopping_criteria,)\n",
    "    response = tokenizer.decode(generate[0].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    # return response[len('[UNUSED_TOKEN_146]assistant\\n'):-len('[UNUSED_TOKEN_145]\\n')]\n",
    "    resp = response.split(\"[UNUSED_TOKEN_145]\", 1)[-1].strip()\n",
    "    if resp.startswith(\"[UNUSED_TOKEN_146]\"):\n",
    "        resp = resp[len(\"[UNUSED_TOKEN_146]\"):].strip()\n",
    "    if \"<bop>\" in resp:\n",
    "        resp = resp.split(\"<bop>\", 1)[-1]\n",
    "    if \"<eop>\" in resp:\n",
    "        resp = resp.split(\"<eop>\", 1)[0]\n",
    "    return resp.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['在北京的心脏', '高耸钟楼的钟声在回荡', '这是飞翔的聚地', '清华大学梦的光芒', '承载知识与智慧', '燃烧着文化的热烈火花', '你的名字是繁荣的象征', '清华大学砥砺前行的力量', '历史的风雨中', '你肩上的责任就是希望', '清华大学你是明灯', '照亮每个孤独的夜晚', '在这片梦想的土地', '冲破黑暗的束缚', '清华大学你是儿女们', '心中的楷模', '向你致敬', '清华大学', '你是我们内心的宇宙', '带着梦想向前行', '清华大学，我们再不曾孤独。']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)\n",
    "print(len(lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# example prompt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# prompt = 'Compose a tune in harmony with the accompanying lyrics. <bol> Total 6 lines.\\\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# The first line:在|那|玉|兰|花|开|的|地|方\\n\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# The fifth line:清|华|大|学|你|的|名|字\\n\\\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# The sixth line:散|发|着|光\\n<eol>'\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m inference(model, tokenizer, \u001b[43mget_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlyrics\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/root/output.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mget_prompt\u001b[0;34m(lyric_list)\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompose a tune in harmony with the accompanying lyrics. <bol> Total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lyric_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lines.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m lyric_list:\n\u001b[0;32m----> 7\u001b[0m     prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_list[lyric_list\u001b[38;5;241m.\u001b[39mindex(item)]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m line:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eol>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "index_list = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\"]\n",
    "\n",
    "# no more than 10 sentences?\n",
    "def get_prompt(lyric_list):\n",
    "    prompt = f'Compose a tune in harmony with the accompanying lyrics. <bol> Total {len(lyric_list)} lines.'\n",
    "    for item in lyric_list:\n",
    "        prompt += f'The {index_list[lyric_list.index(item)]} line:{\"|\".join(item)}\\n'\n",
    "    prompt += '<eol>'\n",
    "    return prompt\n",
    "        \n",
    "# example prompt\n",
    "# prompt = 'Compose a tune in harmony with the accompanying lyrics. <bol> Total 6 lines.\\\n",
    "# The first line:在|那|玉|兰|花|开|的|地|方\\n\\\n",
    "# The second line:有|一|所|赫|赫|有|名|的|学|府\\n\\\n",
    "# The third line:那|是|清|华|我|的|清|华\\n\\\n",
    "# The fourth line:在|你|怀|抱|中|我|学|会|的|飞|翔\\n\\\n",
    "# The fifth line:清|华|大|学|你|的|名|字\\n\\\n",
    "# The sixth line:散|发|着|光\\n<eol>'\n",
    "result = inference(model, tokenizer, get_prompt(lyrics))\n",
    "print(result)\n",
    "with open('/root/output.txt','w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['清', '<C4>', '<144>', '<79>'],\n",
       "  ['华', '<C4>', '<131>', '<79>'],\n",
       "  ['大', '<C4>', '<131>', '<126>'],\n",
       "  ['学', '<C4>', '<103>', '<126>'],\n",
       "  ['智', '<C4>', '<96>', '<79>'],\n",
       "  ['慧', '<C4>', '<145>', '<79>'],\n",
       "  ['的', '<C4>', '<138>', '<79>'],\n",
       "  ['圣', '<C4>', '<141>', '<79>'],\n",
       "  ['殿', '<C4>', '<135>', '<79>']],\n",
       " [['在', '<C4>', '<141>', '<79>'],\n",
       "  ['你', '<C4>', '<135>', '<79>'],\n",
       "  ['的', '<C4>', '<138>', '<79>'],\n",
       "  ['怀', '<C4>', '<141>', '<79>'],\n",
       "  ['抱', '<C4>', '<140>', '<79>'],\n",
       "  ['总', '<C4>', '<140>', '<79>'],\n",
       "  ['是', '<C4>', '<140>', '<79>'],\n",
       "  ['光', '<C4>', '<131>', '<79>'],\n",
       "  ['芒', '<C4>', '<140>', '<79>'],\n",
       "  ['万', '<C4>', '<113>', '<79>']],\n",
       " [['清', '<C4>', '<129>', '<79>'],\n",
       "  ['华', '<C4>', '<97>', '<79>'],\n",
       "  ['大', '<C4>', '<96>', '<79>'],\n",
       "  ['学', '<C4>', '<98>', '<79>'],\n",
       "  ['梦', '<C4>', '<97>', '<79>'],\n",
       "  ['想', '<C4>', '<145>', '<79>'],\n",
       "  ['起', '<C4>', '<140>', '<79>'],\n",
       "  ['航', '<C4>', '<140>', '<79>'],\n",
       "  ['的', '<C4>', '<131>', '<79>'],\n",
       "  ['港', '<C4>', '<97>', '<79>']],\n",
       " [['寻', '<C4>', '<100>', '<79>'],\n",
       "  ['求', '<C4>', '<100>', '<79>'],\n",
       "  ['真', '<C4>', '<134>', '<79>'],\n",
       "  ['知', '<C4>', '<97>', '<79>'],\n",
       "  ['积', '<C4>', '<102>', '<79>'],\n",
       "  ['累', '<C4>', '<99>', '<314>'],\n",
       "  ['深', '<D#4>', '<196>', '<126>'],\n",
       "  ['厚', '<C4>', '<201>', '<226>'],\n",
       "  ['清', '<C4>', '<124>', '<79>'],\n",
       "  ['华', '<C4>', '<100>', '<79>'],\n",
       "  ['大', '<D4>', '<104>', '<79>'],\n",
       "  ['学', '<C4>', '<98>', '<79>'],\n",
       "  ['我', '<D4>', '<126>', '<126>'],\n",
       "  ['们', '<D4>', '<167>', '<226>'],\n",
       "  ['为', '<G4>', '<254>', '<126>'],\n",
       "  ['你', '<C4>', '<141>', '<79>'],\n",
       "  ['歌', '<C4>', '<126>', '<79>'],\n",
       "  ['唱', '<D4>', '<104>', '<79>']],\n",
       " [['韶', '<C4>', '<102>', '<79>'],\n",
       "  ['华', '<D4>', '<124>', '<126>'],\n",
       "  ['短', '<D4>', '<92>', '<79>'],\n",
       "  ['暂', '<D#4>', '<138>', '<79>']],\n",
       " [['我', '<D4>', '<97>', '<79>'],\n",
       "  ['们', '<C4>', '<103>', '<79>'],\n",
       "  ['为', '<D4>', '<97>', '<79>'],\n",
       "  ['你', '<C4>', '<168>', '<79>'],\n",
       "  ['歌', '<D4>', '<97>', '<79>'],\n",
       "  ['唱', '<C4>', '<107>', '<79>'],\n",
       "  ['清', '<D4>', '<102>', '<79>'],\n",
       "  ['华', '<C4>', '<153>', '<79>'],\n",
       "  ['大', '<C4>', '<176>', '<79>'],\n",
       "  ['学', '<C4>', '<94>', '<79>']],\n",
       " [['你', '<C4>', '<173>', '<79>'],\n",
       "  ['是', '<C4>', '<133>', '<79>'],\n",
       "  ['光', '<C4>', '<95>', '<79>'],\n",
       "  ['引', '<C4>', '<110>', '<79>'],\n",
       "  ['领', '<C4>', '<96>', '<79>'],\n",
       "  ['我', '<C4>', '<115>', '<126>'],\n",
       "  ['们', '<C4>', '<96>', '<79>'],\n",
       "  ['前', '<C4>', '<102>', '<79>'],\n",
       "  ['行', '<C4>', '<111>', '<126>'],\n",
       "  ['的', '<C4>', '<111>', '<79>'],\n",
       "  ['星', '<C4>', '<113>', '<165>']],\n",
       " [['光', '<C4>', '<103>', '<165>'],\n",
       "  ['芒', '<C4>', '<108>', '<126>'],\n",
       "  ['万', '<C4>', '<162>', '<79>'],\n",
       "  ['丈', '<C4>', '<105>', '<79>']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = result.split(\"line:\")[1:]\n",
    "result_list = [item.split(\"The\", 1)[0].strip().split(\"|\") for item in result_list]\n",
    "result_list = [[jtem.split(\",\") for jtem in item] for item in result_list]\n",
    "result_list = [[[ktem.strip() for ktem in jtem] for jtem in item] for item in result_list]\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8 lines. The first line:清, <C4> , <144> , <79> \n",
      "华, <C4> , <131> , <79> \n",
      "大, <C4> , <131> , <126> \n",
      "学, <C4> , <103> , <126> \n",
      "智, <C4> , <96> , <79> \n",
      "慧, <C4> , <145> , <79> \n",
      "的, <C4> , <138> , <79> \n",
      "圣, <C4> , <141> , <79> \n",
      "殿, <C4> , <135> , <79> The second line:在, <C4> , <141> , <79> \n",
      "你, <C4> , <135> , <79> \n",
      "的, <C4> , <138> , <79> \n",
      "怀, <C4> , <141> , <79> \n",
      "抱, <C4> , <140> , <79> \n",
      "总, <C4> , <140> , <79> \n",
      "是, <C4> , <140> , <79> \n",
      "光, <C4> , <131> , <79> \n",
      "芒, <C4> , <140> , <79> \n",
      "万, <C4> , <113> , <79> The third line:清, <C4> , <129> , <79> \n",
      "华, <C4> , <97> , <79> \n",
      "大, <C4> , <96> , <79> \n",
      "学, <C4> , <98> , <79> \n",
      "梦, <C4> , <97> , <79> \n",
      "想, <C4> , <145> , <79> \n",
      "起, <C4> , <140> , <79> \n",
      "航, <C4> , <140> , <79> \n",
      "的, <C4> , <131> , <79> \n",
      "港, <C4> , <97> , <79> The fourth line:寻, <C4> , <100> , <79> \n",
      "求, <C4> , <100> , <79> \n",
      "真, <C4> , <134> , <79> \n",
      "知, <C4> , <97> , <79> \n",
      "积, <C4> , <102> , <79> \n",
      "累, <C4> , <99> , <314> \n",
      "深, <D#4> , <196> , <126> \n",
      "厚, <C4> , <201> , <226> \n",
      "清, <C4> , <124> , <79> \n",
      "华, <C4> , <100> , <79> \n",
      "大, <D4> , <104> , <79> \n",
      "学, <C4> , <98> , <79> \n",
      "我, <D4> , <126> , <126> \n",
      "们, <D4> , <167> , <226> \n",
      "为, <G4> , <254> , <126> \n",
      "你, <C4> , <141> , <79> \n",
      "歌, <C4> , <126> , <79> \n",
      "唱, <D4> , <104> , <79> The fifth line:韶, <C4> , <102> , <79> \n",
      "华, <D4> , <124> , <126> \n",
      "短, <D4> , <92> , <79> \n",
      "暂, <D#4> , <138> , <79> The sixth line:我, <D4> , <97> , <79> \n",
      "们, <C4> , <103> , <79> \n",
      "为, <D4> , <97> , <79> \n",
      "你, <C4> , <168> , <79> \n",
      "歌, <D4> , <97> , <79> \n",
      "唱, <C4> , <107> , <79> \n",
      "清, <D4> , <102> , <79> \n",
      "华, <C4> , <153> , <79> \n",
      "大, <C4> , <176> , <79> \n",
      "学, <C4> , <94> , <79> The seventh line:你, <C4> , <173> , <79> \n",
      "是, <C4> , <133> , <79> \n",
      "光, <C4> , <95> , <79> \n",
      "引, <C4> , <110> , <79> \n",
      "领, <C4> , <96> , <79> \n",
      "我, <C4> , <115> , <126> \n",
      "们, <C4> , <96> , <79> \n",
      "前, <C4> , <102> , <79> \n",
      "行, <C4> , <111> , <126> \n",
      "的, <C4> , <111> , <79> \n",
      "星, <C4> , <113> , <165> The eighth line:光, <C4> , <103> , <165> \n",
      "芒, <C4> , <108> , <126> \n",
      "万, <C4> , <162> , <79> \n",
      "丈, <C4> , <105> , <79>\n"
     ]
    }
   ],
   "source": [
    "for item in result.split('|'):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['清, <C4> , <144> , <79> |华, <C4> , <131> , <79> |大, <C4> , <131> , <126> |学, <C4> , <103> , <126> |智, <C4> , <96> , <79> |慧, <C4> , <145> , <79> |的, <C4> , <138> , <79> |圣, <C4> , <141> , <79> |殿, <C4> , <135> , <79> The second ', '在, <C4> , <141> , <79> |你, <C4> , <135> , <79> |的, <C4> , <138> , <79> |怀, <C4> , <141> , <79> |抱, <C4> , <140> , <79> |总, <C4> , <140> , <79> |是, <C4> , <140> , <79> |光, <C4> , <131> , <79> |芒, <C4> , <140> , <79> |万, <C4> , <113> , <79> The third ', '清, <C4> , <129> , <79> |华, <C4> , <97> , <79> |大, <C4> , <96> , <79> |学, <C4> , <98> , <79> |梦, <C4> , <97> , <79> |想, <C4> , <145> , <79> |起, <C4> , <140> , <79> |航, <C4> , <140> , <79> |的, <C4> , <131> , <79> |港, <C4> , <97> , <79> The fourth ', '寻, <C4> , <100> , <79> |求, <C4> , <100> , <79> |真, <C4> , <134> , <79> |知, <C4> , <97> , <79> |积, <C4> , <102> , <79> |累, <C4> , <99> , <314> |深, <D#4> , <196> , <126> |厚, <C4> , <201> , <226> |清, <C4> , <124> , <79> |华, <C4> , <100> , <79> |大, <D4> , <104> , <79> |学, <C4> , <98> , <79> |我, <D4> , <126> , <126> |们, <D4> , <167> , <226> |为, <G4> , <254> , <126> |你, <C4> , <141> , <79> |歌, <C4> , <126> , <79> |唱, <D4> , <104> , <79> The fifth ', '韶, <C4> , <102> , <79> |华, <D4> , <124> , <126> |短, <D4> , <92> , <79> |暂, <D#4> , <138> , <79> The sixth ', '我, <D4> , <97> , <79> |们, <C4> , <103> , <79> |为, <D4> , <97> , <79> |你, <C4> , <168> , <79> |歌, <D4> , <97> , <79> |唱, <C4> , <107> , <79> |清, <D4> , <102> , <79> |华, <C4> , <153> , <79> |大, <C4> , <176> , <79> |学, <C4> , <94> , <79> The seventh ', '你, <C4> , <173> , <79> |是, <C4> , <133> , <79> |光, <C4> , <95> , <79> |引, <C4> , <110> , <79> |领, <C4> , <96> , <79> |我, <C4> , <115> , <126> |们, <C4> , <96> , <79> |前, <C4> , <102> , <79> |行, <C4> , <111> , <126> |的, <C4> , <111> , <79> |星, <C4> , <113> , <165> The eighth ', '光, <C4> , <103> , <165> |芒, <C4> , <108> , <126> |万, <C4> , <162> , <79> |丈, <C4> , <105> , <79>']\n",
      "['清, <C4> , <144> , <79> |华, <C4> , <131> , <79> |大, <C4> , <131> , <126> |学, <C4> , <103> , <126> |智, <C4> , <96> , <79> |慧, <C4> , <145> , <79> |的, <C4> , <138> , <79> |圣, <C4> , <141> , <79> |殿, <C4> , <135> , <79>']\n",
      "['在, <C4> , <141> , <79> |你, <C4> , <135> , <79> |的, <C4> , <138> , <79> |怀, <C4> , <141> , <79> |抱, <C4> , <140> , <79> |总, <C4> , <140> , <79> |是, <C4> , <140> , <79> |光, <C4> , <131> , <79> |芒, <C4> , <140> , <79> |万, <C4> , <113> , <79>']\n",
      "['清, <C4> , <129> , <79> |华, <C4> , <97> , <79> |大, <C4> , <96> , <79> |学, <C4> , <98> , <79> |梦, <C4> , <97> , <79> |想, <C4> , <145> , <79> |起, <C4> , <140> , <79> |航, <C4> , <140> , <79> |的, <C4> , <131> , <79> |港, <C4> , <97> , <79>']\n",
      "['寻, <C4> , <100> , <79> |求, <C4> , <100> , <79> |真, <C4> , <134> , <79> |知, <C4> , <97> , <79> |积, <C4> , <102> , <79> |累, <C4> , <99> , <314> |深, <D#4> , <196> , <126> |厚, <C4> , <201> , <226> |清, <C4> , <124> , <79> |华, <C4> , <100> , <79> |大, <D4> , <104> , <79> |学, <C4> , <98> , <79> |我, <D4> , <126> , <126> |们, <D4> , <167> , <226> |为, <G4> , <254> , <126> |你, <C4> , <141> , <79> |歌, <C4> , <126> , <79> |唱, <D4> , <104> , <79>']\n",
      "['韶, <C4> , <102> , <79> |华, <D4> , <124> , <126> |短, <D4> , <92> , <79> |暂, <D#4> , <138> , <79>']\n",
      "['我, <D4> , <97> , <79> |们, <C4> , <103> , <79> |为, <D4> , <97> , <79> |你, <C4> , <168> , <79> |歌, <D4> , <97> , <79> |唱, <C4> , <107> , <79> |清, <D4> , <102> , <79> |华, <C4> , <153> , <79> |大, <C4> , <176> , <79> |学, <C4> , <94> , <79>']\n",
      "['你, <C4> , <173> , <79> |是, <C4> , <133> , <79> |光, <C4> , <95> , <79> |引, <C4> , <110> , <79> |领, <C4> , <96> , <79> |我, <C4> , <115> , <126> |们, <C4> , <96> , <79> |前, <C4> , <102> , <79> |行, <C4> , <111> , <126> |的, <C4> , <111> , <79> |星, <C4> , <113> , <165>']\n",
      "['光, <C4> , <103> , <165> |芒, <C4> , <108> , <126> |万, <C4> , <162> , <79> |丈, <C4> , <105> , <79>']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = open('/root/output.txt').read()\n",
    "lines = s.split('line:')[1:]\n",
    "print(lines)\n",
    "L = [[],[],[],[]]\n",
    "for line in lines:\n",
    "    line += ' '\n",
    "    line_p = re.findall('^(.+>)[^>]+$',line)\n",
    "    print(line_p)\n",
    "    sentences = line_p[0].split(' |')\n",
    "    for sentence in sentences:\n",
    "        results = re.findall(\n",
    "            '^([^<>]+)(<.+>)[^<>]+(<.+>)[^<>]+(<.+>)$',sentence\n",
    "        )\n",
    "        if len(results)>0:\n",
    "            for i,x in enumerate(results[0]):\n",
    "                if i>0:\n",
    "                    L[i].append(x)\n",
    "                else:\n",
    "                    L[i].append(x[0])\n",
    "        else:\n",
    "            results = re.findall(\n",
    "            '^([^<>]+)(<.+>)[^<>]+(<.+>)[^<>]+$',sentence\n",
    "            )\n",
    "            if len(results)>0:\n",
    "                for i,x in enumerate(results[0]):\n",
    "                    if i>0:\n",
    "                        L[i].append(x)\n",
    "                    else:\n",
    "                        L[i].append(x[0])\n",
    "                # print(L[1])\n",
    "                L[0].append('<?>')\n",
    "            else:\n",
    "                print('!!! not handled: ',sentence)\n",
    "    for i in L:\n",
    "        i.append(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: 84\n",
      "second: 84\n",
      "third: 84\n",
      "fourth: 84\n",
      "['清', '华', '大', '学', '智', '慧', '的', '圣', '殿', ',', '在', '你', '的', '怀', '抱', '总', '是', '光', '芒', '万', ',', '清', '华', '大', '学', '梦', '想', '起', '航', '的', '港', ',', '寻', '求', '真', '知', '积', '累', '深', '厚', '清', '华', '大', '学', '我', '们', '为', '你', '歌', '唱', ',', '韶', '华', '短', '暂', ',', '我', '们', '为', '你', '歌', '唱', '清', '华', '大', '学', ',', '你', '是', '光', '引', '领', '我', '们', '前', '行', '的', '星', ',', '光', '芒', '万', '丈', ',']\n",
      "['<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<D#4>', '<C4>', '<C4>', '<C4>', '<D4>', '<C4>', '<D4>', '<D4>', '<G4>', '<C4>', '<C4>', '<D4>', ',', '<C4>', '<D4>', '<D4>', '<D#4>', ',', '<D4>', '<C4>', '<D4>', '<C4>', '<D4>', '<C4>', '<D4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', ',']\n",
      "['<144>', '<131>', '<131>', '<103>', '<96>', '<145>', '<138>', '<141>', '<135>', ',', '<141>', '<135>', '<138>', '<141>', '<140>', '<140>', '<140>', '<131>', '<140>', '<113>', ',', '<129>', '<97>', '<96>', '<98>', '<97>', '<145>', '<140>', '<140>', '<131>', '<97>', ',', '<100>', '<100>', '<134>', '<97>', '<102>', '<99>', '<196>', '<201>', '<124>', '<100>', '<104>', '<98>', '<126>', '<167>', '<254>', '<141>', '<126>', '<104>', ',', '<102>', '<124>', '<92>', '<138>', ',', '<97>', '<103>', '<97>', '<168>', '<97>', '<107>', '<102>', '<153>', '<176>', '<94>', ',', '<173>', '<133>', '<95>', '<110>', '<96>', '<115>', '<96>', '<102>', '<111>', '<111>', '<113>', ',', '<103>', '<108>', '<162>', '<105>', ',']\n",
      "['<79>', '<79>', '<126>', '<126>', '<79>', '<79>', '<79>', '<79>', '<79>', ',', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', ',', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', ',', '<79>', '<79>', '<79>', '<79>', '<79>', '<314>', '<126>', '<226>', '<79>', '<79>', '<79>', '<79>', '<126>', '<226>', '<126>', '<79>', '<79>', '<79>', ',', '<79>', '<126>', '<79>', '<79>', ',', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', '<79>', ',', '<79>', '<79>', '<79>', '<79>', '<79>', '<126>', '<79>', '<79>', '<126>', '<79>', '<165>', ',', '<165>', '<126>', '<79>', '<79>', ',']\n",
      "res_pitch C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|D#4|rest|C4|rest|C4|rest|C4|rest|D4|rest|C4|rest|D4|rest|D4|rest|G4|rest|C4|rest|C4|rest|D4|rest|C4|rest|D4|rest|D4|rest|D#4|rest|D4|rest|C4|rest|D4|rest|C4|rest|D4|rest|C4|rest|D4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest|C4|rest\n",
      "res_time 0.4715|0.0395|0.4325|0.0395|0.456|0.063|0.372|0.063|0.32749999999999996|0.0395|0.4745|0.0395|0.45349999999999996|0.0395|0.46249999999999997|0.0395|0.4445|0.0395|0.46249999999999997|0.0395|0.4445|0.0395|0.45349999999999996|0.0395|0.46249999999999997|0.0395|0.45949999999999996|0.0395|0.45949999999999996|0.0395|0.45949999999999996|0.0395|0.4325|0.0395|0.45949999999999996|0.0395|0.3785|0.0395|0.4265|0.0395|0.33049999999999996|0.0395|0.32749999999999996|0.0395|0.33349999999999996|0.0395|0.33049999999999996|0.0395|0.4745|0.0395|0.45949999999999996|0.0395|0.45949999999999996|0.0395|0.4325|0.0395|0.33049999999999996|0.0395|0.33949999999999997|0.0395|0.33949999999999997|0.0395|0.4415|0.0395|0.33049999999999996|0.0395|0.3455|0.0395|0.45399999999999996|0.157|0.651|0.063|0.716|0.113|0.4115|0.0395|0.33949999999999997|0.0395|0.3515|0.0395|0.33349999999999996|0.0395|0.441|0.063|0.614|0.113|0.825|0.063|0.46249999999999997|0.0395|0.4175|0.0395|0.3515|0.0395|0.3455|0.0395|0.435|0.063|0.3155|0.0395|0.45349999999999996|0.0395|0.33049999999999996|0.0395|0.3485|0.0395|0.33049999999999996|0.0395|0.5435|0.0395|0.33049999999999996|0.0395|0.3605|0.0395|0.3455|0.0395|0.4985|0.0395|0.5675|0.0395|0.32149999999999995|0.0395|0.5585|0.0395|0.4385|0.0395|0.32449999999999996|0.0395|0.3695|0.0395|0.32749999999999996|0.0395|0.408|0.063|0.32749999999999996|0.0395|0.3455|0.0395|0.396|0.063|0.3725|0.0395|0.42150000000000004|0.0825|0.3915|0.0825|0.387|0.063|0.5255|0.0395|0.3545|0.0395\n",
      "lyric: 清AP华AP大AP学AP智AP慧AP的AP圣AP殿AP在AP你AP的AP怀AP抱AP总AP是AP光AP芒AP万AP清AP华AP大AP学AP梦AP想AP起AP航AP的AP港AP寻AP求AP真AP知AP积AP累AP深AP厚AP清AP华AP大AP学AP我AP们AP为AP你AP歌AP唱AP韶AP华AP短AP暂AP我AP们AP为AP你AP歌AP唱AP清AP华AP大AP学AP你AP是AP光AP引AP领AP我AP们AP前AP行AP的AP星AP光AP芒AP万AP丈AP\n",
      "228\n",
      "152\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print('first:',len(L[0]))\n",
    "print('second:',len(L[1]))\n",
    "print('third:',len(L[2]))\n",
    "print('fourth:',len(L[3]))\n",
    "print(L[0])\n",
    "print(L[1])\n",
    "print(L[2])\n",
    "print(L[3])\n",
    "# for i,x in enumerate(L):\n",
    "#     if(i == 0):\n",
    "#         L[i]='AP'.join(x)\n",
    "#     if(i == 2):\n",
    "#         timing = ''.join(x).split('>')\n",
    "\n",
    "# new_L2 = []\n",
    "# for item in timing:\n",
    "#     item = item.replace('<','').replace(',','')\n",
    "#     if(item==''):\n",
    "#         continue\n",
    "#     it = float(item)\n",
    "#     it = it * 3 / 1000\n",
    "#     new_L2.append(it)   \n",
    "# res = []\n",
    "# res = '|'.join(str(x) for x in new_L2)\n",
    "res_pitch = []\n",
    "res_time = []\n",
    "res_lyric = []\n",
    "for i in range(len(L[1])):\n",
    "    if(L[1][i] == ','):\n",
    "        continue\n",
    "    res_pitch.append(L[1][i].replace('<','').replace('>',''))\n",
    "    res_pitch.append('rest')\n",
    "    yinfu_time = float(L[2][i].replace('<','').replace('>','')) * 3 / 1000\n",
    "    rest_time = float(L[3][i].replace('<','').replace('>','')) / 1000\n",
    "    res_time.append(str(yinfu_time+rest_time/2))\n",
    "    res_time.append(str(rest_time/2))\n",
    "res_pitch = '|'.join(x for x in res_pitch)\n",
    "res_time = '|'.join(str(x) for x in res_time)\n",
    "# L[0].replace(',','')\n",
    "for item in L[0]:\n",
    "    if(item == ','):\n",
    "        continue\n",
    "    res_lyric.append(item)\n",
    "res_lyric = 'AP'.join(x for x in res_lyric) + 'AP'\n",
    "print('res_pitch',res_pitch)\n",
    "print('res_time',res_time)\n",
    "print('lyric:',res_lyric)\n",
    "print(len(res_lyric))\n",
    "print(len(res_pitch.split('|')))\n",
    "print(len(res_time.split('|')))\n",
    "# print('second:',L[1])\n",
    "# print('third:',L[2])\n",
    "# print('fourth:',L[3])\n",
    "# print('new_L2', new_L2)\n",
    "# print('res:',res)\n",
    "file_path_lyric = '/root/lyric_output.txt'\n",
    "file_path_pitch = '/root/pitch_output.txt'\n",
    "file_path_time = '/root/time_output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['清', '华', '大', '学', '智', '慧', '的', '圣', '殿', ',', '在', '你', '的', '怀', '抱', '总', '是', '光', '芒', '万', ',', '清', '华', '大', '学', '梦', '想', '起', '航', '的', '港', ',', '寻', '求', '真', '知', '积', '累', '深', '厚', '清', '华', '大', '学', '我', '们', '为', '你', '歌', '唱', ',', '韶', '华', '短', '暂', ',', '我', '们', '为', '你', '歌', '唱', '清', '华', '大', '学', ',', '你', '是', '光', '引', '领', '我', '们', '前', '行', '的', '星', ',', '光', '芒', '万', '丈', ',']\n",
      "['<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<D#4>', '<C4>', '<C4>', '<C4>', '<D4>', '<C4>', '<D4>', '<D4>', '<G4>', '<C4>', '<C4>', '<D4>', ',', '<C4>', '<D4>', '<D4>', '<D#4>', ',', '<D4>', '<C4>', '<D4>', '<C4>', '<D4>', '<C4>', '<D4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', '<C4>', ',', '<C4>', '<C4>', '<C4>', '<C4>', ',']\n",
      "['<144>', '<131>', '<131>', '<103>', '<96>', '<145>', '<138>', '<141>', '<135>', ',', '<141>', '<135>', '<138>', '<141>', '<140>', '<140>', '<140>', '<131>', '<140>', '<113>', ',', '<129>', '<97>', '<96>', '<98>', '<97>', '<145>', '<140>', '<140>', '<131>', '<97>', ',', '<100>', '<100>', '<134>', '<97>', '<102>', '<99>', '<196>', '<201>', '<124>', '<100>', '<104>', '<98>', '<126>', '<167>', '<254>', '<141>', '<126>', '<104>', ',', '<102>', '<124>', '<92>', '<138>', ',', '<97>', '<103>', '<97>', '<168>', '<97>', '<107>', '<102>', '<153>', '<176>', '<94>', ',', '<173>', '<133>', '<95>', '<110>', '<96>', '<115>', '<96>', '<102>', '<111>', '<111>', '<113>', ',', '<103>', '<108>', '<162>', '<105>', ',']\n",
      "84\n",
      "['清', '华', '大', '学', '智', '慧', '的', '圣', '殿', 'AP', '在', '你', '的', '怀', '抱', '总', '是', '光', '芒', '万', 'AP', '清', '华', '大', '学', '梦', '想', '起', '航', '的', '港', 'AP', '寻', '求', '真', '知', '积', '累', '深', '厚', '清', '华', '大', '学', '我', '们', '为', '你', '歌', '唱', 'AP', '韶', '华', '短', '暂', 'AP', '我', '们', '为', '你', '歌', '唱', '清', '华', '大', '学', 'AP', '你', '是', '光', '引', '领', '我', '们', '前', '行', '的', '星', 'AP', '光', '芒', '万', '丈', 'AP']\n",
      "['C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'rest', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'rest', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'rest', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'D#4', 'C4', 'C4', 'C4', 'D4', 'C4', 'D4', 'D4', 'G4', 'C4', 'C4', 'D4', 'rest', 'C4', 'D4', 'D4', 'D#4', 'rest', 'D4', 'C4', 'D4', 'C4', 'D4', 'C4', 'D4', 'C4', 'C4', 'C4', 'rest', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'rest', 'C4', 'C4', 'C4', 'C4', 'rest']\n",
      "['0.432', '0.393', '0.393', '0.309', '0.288', '0.435', '0.414', '0.423', '0.405', '0.08944444444444444', '0.423', '0.405', '0.414', '0.423', '0.42', '0.42', '0.42', '0.393', '0.42', '0.339', '0.07899999999999999', '0.387', '0.291', '0.288', '0.294', '0.291', '0.435', '0.42', '0.42', '0.393', '0.291', '0.07899999999999999', '0.3', '0.3', '0.402', '0.291', '0.306', '0.297', '0.588', '0.603', '0.372', '0.3', '0.312', '0.294', '0.378', '0.501', '0.762', '0.423', '0.378', '0.312', '0.11622222222222223', '0.306', '0.372', '0.276', '0.414', '0.09075000000000001', '0.291', '0.309', '0.291', '0.504', '0.291', '0.321', '0.306', '0.459', '0.528', '0.282', '0.07899999999999999', '0.519', '0.399', '0.285', '0.33', '0.288', '0.345', '0.288', '0.306', '0.333', '0.333', '0.339', '0.09536363636363636', '0.309', '0.324', '0.486', '0.315', '0.11225000000000002']\n",
      "84\n",
      "84\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "#another kind of inference\n",
    "# for i,x in enumerate(L):\n",
    "#     L[i]=''.join(x)\n",
    "print(L[0])\n",
    "print(L[1])\n",
    "print(L[2])\n",
    "print(len(L[1]))\n",
    "res_lyric = []\n",
    "res_pitch = []\n",
    "res_time = []\n",
    "cnt = 0.0\n",
    "flag = 0 \n",
    "for i in range(len(L[1])):\n",
    "    if(L[1][i] == ','):\n",
    "        res_pitch.append('rest')\n",
    "        res_time.append(str(cnt/flag))\n",
    "        cnt = 0.0\n",
    "        flag = 0\n",
    "        continue\n",
    "    res_pitch.append(L[1][i].replace('<','').replace('>',''))\n",
    "    yinfu_time = float(L[2][i].replace('<','').replace('>','')) * 3 / 1000\n",
    "    rest_time = float(L[3][i].replace('<','').replace('>','')) / 1000\n",
    "    cnt += rest_time\n",
    "    flag += 1\n",
    "    res_time.append(str(yinfu_time))\n",
    "    # print(L[0][i])\n",
    "    # if(L[0][i] == ','):\n",
    "        \n",
    "for item in L[0]:\n",
    "    if(item == ','):\n",
    "        res_lyric.append('AP')\n",
    "        continue\n",
    "    res_lyric.append(item)\n",
    "print(res_lyric)\n",
    "print(res_pitch)\n",
    "print(res_time)\n",
    "print(len(res_lyric))\n",
    "print(len(res_pitch))\n",
    "print(len(res_time))\n",
    "res_lyric = ''.join(x for x in res_lyric)\n",
    "res_pitch = '|'.join(x for x in res_pitch)\n",
    "res_time = '|'.join(str(x) for x in res_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path_lyric, 'w') as f:\n",
    "    f.write(res_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path_pitch, 'w') as f:\n",
    "    f.write(res_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path_time, 'w') as f:\n",
    "    f.write(res_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清华大学智慧的圣殿AP在你的怀抱总是光芒万AP清华大学梦想起航的港AP寻求真知积累深厚清华大学我们为你歌唱AP韶华短暂AP我们为你歌唱清华大学AP你是光引领我们前行的星AP光芒万丈AP\n",
      "C4|C4|C4|C4|C4|C4|C4|C4|C4|rest|C4|C4|C4|C4|C4|C4|C4|C4|C4|C4|rest|C4|C4|C4|C4|C4|C4|C4|C4|C4|C4|rest|C4|C4|C4|C4|C4|C4|D#4|C4|C4|C4|D4|C4|D4|D4|G4|C4|C4|D4|rest|C4|D4|D4|D#4|rest|D4|C4|D4|C4|D4|C4|D4|C4|C4|C4|rest|C4|C4|C4|C4|C4|C4|C4|C4|C4|C4|C4|rest|C4|C4|C4|C4|rest\n",
      "0.432|0.393|0.393|0.309|0.288|0.435|0.414|0.423|0.405|0.08944444444444444|0.423|0.405|0.414|0.423|0.42|0.42|0.42|0.393|0.42|0.339|0.07899999999999999|0.387|0.291|0.288|0.294|0.291|0.435|0.42|0.42|0.393|0.291|0.07899999999999999|0.3|0.3|0.402|0.291|0.306|0.297|0.588|0.603|0.372|0.3|0.312|0.294|0.378|0.501|0.762|0.423|0.378|0.312|0.11622222222222223|0.306|0.372|0.276|0.414|0.09075000000000001|0.291|0.309|0.291|0.504|0.291|0.321|0.306|0.459|0.528|0.282|0.07899999999999999|0.519|0.399|0.285|0.33|0.288|0.345|0.288|0.306|0.333|0.333|0.339|0.09536363636363636|0.309|0.324|0.486|0.315|0.11225000000000002\n"
     ]
    }
   ],
   "source": [
    "print(res_lyric)\n",
    "print(res_pitch)\n",
    "print(res_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/root/DiffSinger')\n",
    "os.environ['PYTHONPATH'] = '/root/DiffSinger'\n",
    "os.environ['MY_DS_EXP_NAME'] = '0228_opencpop_ds100_rel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE      data\t     infer_out\t readme.md\t\tusr\n",
      "README.md    data_gen\t     inference\t requirements_3090.txt\tutils\n",
      "checkpoints  diffsinger.zip  modules\t resources\t\tvocoders\n",
      "configs      docs\t     output.log  tasks\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /root/DiffSinger/inference/svs/my_infer.py\n"
     ]
    }
   ],
   "source": [
    "%%file /root/DiffSinger/inference/svs/my_infer.py\n",
    "import torch\n",
    "from inference.svs.base_svs_infer import BaseSVSInfer\n",
    "from utils import load_ckpt\n",
    "from utils.hparams import hparams\n",
    "from usr.diff.shallow_diffusion_tts import GaussianDiffusion\n",
    "from usr.diffsinger_task import DIFF_DECODERS\n",
    "from modules.fastspeech.pe import PitchExtractor\n",
    "import utils\n",
    "\n",
    "class DiffSingerE2EInfer(BaseSVSInfer):\n",
    "    def build_model(self):\n",
    "        model = GaussianDiffusion(\n",
    "            phone_encoder=self.ph_encoder,\n",
    "            out_dims=hparams['audio_num_mel_bins'], denoise_fn=DIFF_DECODERS[hparams['diff_decoder_type']](hparams),\n",
    "            timesteps=hparams['timesteps'],\n",
    "            K_step=hparams['K_step'],\n",
    "            loss_type=hparams['diff_loss_type'],\n",
    "            spec_min=hparams['spec_min'], spec_max=hparams['spec_max'],\n",
    "        )\n",
    "        model.eval()\n",
    "        load_ckpt(model, hparams['work_dir'], 'model')\n",
    "\n",
    "        if hparams.get('pe_enable') is not None and hparams['pe_enable']:\n",
    "            self.pe = PitchExtractor().to(self.device)\n",
    "            utils.load_ckpt(self.pe, hparams['pe_ckpt'], 'model', strict=True)\n",
    "            self.pe.eval()\n",
    "        return model\n",
    "\n",
    "    def forward_model(self, inp):\n",
    "        sample = self.input_to_batch(inp)\n",
    "        txt_tokens = sample['txt_tokens']  # [B, T_t]\n",
    "        spk_id = sample.get('spk_ids')\n",
    "        with torch.no_grad():\n",
    "            output = self.model(txt_tokens, spk_id=spk_id, ref_mels=None, infer=True,\n",
    "                                pitch_midi=sample['pitch_midi'], midi_dur=sample['midi_dur'],\n",
    "                                is_slur=sample['is_slur'])\n",
    "            mel_out = output['mel_out']  # [B, T,80]\n",
    "            if hparams.get('pe_enable') is not None and hparams['pe_enable']:\n",
    "                f0_pred = self.pe(mel_out)['f0_denorm_pred']  # pe predict from Pred mel\n",
    "            else:\n",
    "                f0_pred = output['f0_denorm']\n",
    "            wav_out = self.run_vocoder(mel_out, f0=f0_pred)\n",
    "        wav_out = wav_out.cpu().numpy()\n",
    "        return wav_out[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file1 = open('/root/lyric_output.txt', 'r')\n",
    "    file2 = open('/root/pitch_output.txt', 'r')\n",
    "    file3 = open('/root/time_output.txt', 'r')\n",
    "    lyric = file1.read()\n",
    "    pitch = file2.read()\n",
    "    time = file3.read()\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    file3.close()\n",
    "\n",
    "    inp = {\n",
    "        'text': lyric,\n",
    "        'notes': pitch,\n",
    "        'notes_duration': time,\n",
    "        'input_type': 'word'\n",
    "    }\n",
    "    DiffSingerE2EInfer.example_run(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| load 'model' from 'checkpoints/0228_opencpop_ds100_rel/model_ckpt_steps_160000.ckpt'.\n",
      "| load 'model' from 'checkpoints/0102_xiaoma_pe/model_ckpt_steps_60000.ckpt'.\n",
      "| load HifiGAN:  checkpoints/0109_hifigan_bigpopcs_hop128/model_ckpt_steps_280000.ckpt\n",
      "Removing weight norm...\n",
      "Pass word-notes check.\n",
      "160 160 160\n",
      "Pass word-notes check.\n",
      "===> gaussion start.\n",
      "sample time step: 100%|██████████████████████| 100/100 [00:00<00:00, 127.53it/s]\n"
     ]
    }
   ],
   "source": [
    "!python inference/svs/my_infer.py --config usr/configs/midi/e2e/opencpop/ds100_adj_rel.yaml --exp_name 0228_opencpop_ds100_rel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
